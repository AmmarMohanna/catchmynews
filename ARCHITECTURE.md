# ğŸ—ï¸ NewsCatcher Architecture

This document provides a detailed overview of NewsCatcher's architecture, components, and data flow.

## System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER INTERFACE                          â”‚
â”‚                      (Streamlit Frontend)                       â”‚
â”‚                         Port: 8501                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ HTTP/REST
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         API LAYER                               â”‚
â”‚                        (FastAPI)                                â”‚
â”‚                         Port: 8000                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   URLs   â”‚  â”‚ Criteria â”‚  â”‚ Articles â”‚  â”‚ Scraping â”‚      â”‚
â”‚  â”‚ Endpointsâ”‚  â”‚Endpoints â”‚  â”‚Endpoints â”‚  â”‚Endpoints â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                     â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
     â–¼                     â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚PostgreSQLâ”‚          â”‚  Background Jobs  â”‚
â”‚Database â”‚          â”‚  (Celery Worker)  â”‚
â”‚Port:5432â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
     â–²                        â–¼
     â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚              â”‚  Redis (Broker)   â”‚
     â”‚              â”‚   Port: 6379      â”‚
     â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                        â”‚
     â”‚                        â–¼
     â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚              â”‚  Scraping Service â”‚
     â”‚              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
     â”‚              â”‚  â”‚Web Crawler  â”‚  â”‚
     â”‚              â”‚  â”‚Subdomain    â”‚  â”‚
     â”‚              â”‚  â”‚Discovery    â”‚  â”‚
     â”‚              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
     â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                        â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   OpenAI API     â”‚
                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚ â”‚Summarization â”‚ â”‚
                    â”‚ â”‚Categorizationâ”‚ â”‚
                    â”‚ â”‚Relevance     â”‚ â”‚
                    â”‚ â”‚Scoring       â”‚ â”‚
                    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Components

### 1. Frontend (Streamlit)

**Technology**: Python Streamlit
**Port**: 8501
**Container**: `newscatcher_frontend`

**Responsibilities**:
- User interface for all interactions
- Real-time updates and statistics
- Article visualization and filtering
- URL and criteria management
- Theme customization

**Key Features**:
- ğŸ“° News Feed with filtering
- ğŸ”— URL Management
- ğŸ¯ Criteria Management
- ğŸ¨ Multiple color themes
- ğŸ“Š Real-time statistics

**Files**:
- `frontend/app.py` - Main Streamlit application

### 2. Backend API (FastAPI)

**Technology**: Python FastAPI
**Port**: 8000
**Container**: `newscatcher_backend`

**Responsibilities**:
- RESTful API endpoints
- Request validation (Pydantic)
- Database operations (SQLAlchemy)
- Task queue management
- Business logic orchestration

**API Endpoints**:

| Category | Endpoint | Method | Description |
|----------|----------|--------|-------------|
| Health | `/` | GET | Health check |
| URLs | `/urls` | GET | List all URLs |
| | `/urls` | POST | Add new URL |
| | `/urls/{id}` | DELETE | Delete URL |
| | `/urls/{id}/toggle` | PATCH | Toggle active status |
| Criteria | `/criteria` | GET | List criteria |
| | `/criteria` | POST | Create criteria |
| | `/criteria/{id}` | PATCH | Update criteria |
| | `/criteria/{id}` | DELETE | Delete criteria |
| | `/criteria/suggestions` | GET | AI suggestions |
| Articles | `/articles` | GET | List articles (filtered) |
| | `/articles/{id}` | GET | Get article details |
| | `/articles/mark-seen` | POST | Mark as seen |
| Scraping | `/scrape` | POST | Trigger scraping |
| | `/scraping-jobs` | GET | Job history |
| Stats | `/stats` | GET | Statistics |

**Files**:
- `backend/app/main.py` - FastAPI application
- `backend/app/api/` - API schemas and routes
- `backend/app/database.py` - Database configuration

### 3. Database (PostgreSQL)

**Technology**: PostgreSQL 15
**Port**: 5432
**Container**: `newscatcher_postgres`

**Schema**:

```sql
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      urls       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id (PK)         â”‚
â”‚ url             â”‚
â”‚ domain          â”‚
â”‚ is_subdomain    â”‚
â”‚ parent_url_id   â”‚
â”‚ is_active       â”‚
â”‚ created_at      â”‚
â”‚ last_scraped_at â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 1:N
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    articles     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id (PK)         â”‚
â”‚ url             â”‚
â”‚ title           â”‚
â”‚ content         â”‚
â”‚ summary         â”‚
â”‚ source_url_id(FK)â”‚
â”‚ categories      â”‚
â”‚ tags            â”‚
â”‚ relevance_scoresâ”‚
â”‚ published_at    â”‚
â”‚ scraped_at      â”‚
â”‚ is_seen         â”‚
â”‚ is_active       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    criteria     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id (PK)         â”‚
â”‚ name            â”‚
â”‚ description     â”‚
â”‚ keywords        â”‚
â”‚ prompt          â”‚
â”‚ is_active       â”‚
â”‚ created_at      â”‚
â”‚ usage_count     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ scraping_jobs   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id (PK)         â”‚
â”‚ url_id (FK)     â”‚
â”‚ status          â”‚
â”‚ celery_task_id  â”‚
â”‚ pages_scraped   â”‚
â”‚ articles_found  â”‚
â”‚ subdomains_foundâ”‚
â”‚ error_message   â”‚
â”‚ started_at      â”‚
â”‚ completed_at    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Files**:
- `backend/app/models/` - SQLAlchemy models

### 4. Cache & Message Broker (Redis)

**Technology**: Redis 7
**Port**: 6379
**Container**: `newscatcher_redis`

**Usage**:
- **Database 0**: General caching
- **Database 1**: Celery message broker
- **Database 2**: Celery result backend

**Cached Data**:
- Scraped content (temporary)
- API responses
- Session data

**Files**: N/A (configured in `docker-compose.yml`)

### 5. Background Worker (Celery)

**Technology**: Celery with Redis broker
**Container**: `newscatcher_celery`

**Tasks**:

| Task | Description | Trigger |
|------|-------------|---------|
| `scrape_url_task` | Scrape single URL | API call or scheduled |
| `scrape_all_urls_task` | Scrape all URLs | UPDATE button |
| `calculate_relevance_scores_task` | Update relevance scores | After criteria change |

**Task Flow**:

```
User clicks UPDATE
        â–¼
API POST /scrape
        â–¼
Queue scrape_all_urls_task
        â–¼
For each URL:
  Queue scrape_url_task
        â–¼
Celery Worker executes:
  1. Discover subdomains
  2. Scrape pages
  3. Extract articles
  4. Call AI service
  5. Save to database
        â–¼
Update scraping_job status
```

**Configuration**:
- Concurrency: 4 workers
- Time limit: 1 hour per task
- Retry policy: No automatic retries (manual re-scrape)

**Files**:
- `backend/app/celery_worker.py` - Task definitions

### 6. Services

#### 6.1 Scraping Service

**File**: `backend/app/services/scraper.py`

**Class**: `WebScraper`

**Methods**:
- `discover_subdomains(url)` - Find subdomains via link crawling
- `scrape_website(url, max_depth, max_pages)` - Deep scraping
- `_scrape_recursive()` - Recursive link following
- `_fetch_url()` - HTTP request with retry
- `_extract_article()` - Content extraction from HTML
- `_apply_rate_limit()` - Rate limiting per domain

**Features**:
- Async/await for performance
- Respects rate limits (1 second default)
- Extracts meaningful content only
- Handles redirects and errors gracefully
- User-Agent spoofing

#### 6.2 AI Service

**File**: `backend/app/services/ai_service.py`

**Class**: `AIService`

**Methods**:
- `summarize_article(title, content)` - Generate summary
- `categorize_article(title, content)` - Extract categories & tags
- `match_criteria(article, criteria)` - Calculate relevance (0-1)
- `suggest_criteria(articles)` - Generate criteria suggestions
- `batch_process_articles(articles)` - Bulk processing

**OpenAI API Usage**:
- Model: `gpt-4o-mini` (fast & cost-effective)
- Temperature: 0.1-0.7 (depending on task)
- Max tokens: Limited per task

**Cost Optimization**:
- Content truncation (max 3000 chars for summarization)
- Batch processing where possible
- Caching of results

## Data Flow

### Scraping Flow

```
1. User adds URL
   â””â”€â–¶ Save to database (urls table)

2. User clicks UPDATE
   â””â”€â–¶ API: POST /scrape
       â””â”€â–¶ Queue: scrape_all_urls_task
           â””â”€â–¶ For each URL:
               â””â”€â–¶ Queue: scrape_url_task

3. Celery Worker processes task
   â”œâ”€â–¶ Discover subdomains (if main domain)
   â”‚   â””â”€â–¶ Save subdomains to database
   â”œâ”€â–¶ Scrape website (max depth & pages)
   â”‚   â”œâ”€â–¶ Fetch HTML
   â”‚   â”œâ”€â–¶ Extract articles
   â”‚   â””â”€â–¶ Follow internal links
   â”œâ”€â–¶ AI Processing
   â”‚   â”œâ”€â–¶ Summarize each article
   â”‚   â”œâ”€â–¶ Categorize & tag
   â”‚   â””â”€â–¶ Calculate relevance scores
   â””â”€â–¶ Save articles to database

4. Frontend polls for updates
   â””â”€â–¶ Display new articles in News Feed
```

### Filtering Flow

```
1. User selects criteria
   â””â”€â–¶ Frontend: Update filter state

2. Fetch articles with filter
   â””â”€â–¶ API: GET /articles?criteria_id=X&min_relevance=0.5

3. Backend filters articles
   â””â”€â–¶ Query database
   â””â”€â–¶ Filter by relevance_scores JSON field
   â””â”€â–¶ Return filtered results

4. Frontend displays articles
   â””â”€â–¶ Render article cards
   â””â”€â–¶ Show relevance score
   â””â”€â–¶ Highlight unseen articles
```

## Scalability Considerations

### Current Limitations

- Single Celery worker (4 concurrent tasks)
- In-memory rate limiting (lost on restart)
- Simple relevance filtering (not indexed)

### Scaling Options

1. **Horizontal Scaling**:
   - Add more Celery workers: `docker-compose scale celery_worker=5`
   - Use load balancer for API (nginx/HAProxy)
   - Add read replicas for PostgreSQL

2. **Performance Optimization**:
   - Add database indexes on frequently queried fields
   - Implement Elasticsearch for full-text search
   - Use Redis caching more aggressively
   - Implement pagination for large result sets

3. **Advanced Features**:
   - Scheduled scraping (Celery Beat)
   - Incremental scraping (only new content)
   - Vector embeddings for semantic search
   - Real-time notifications (WebSockets)

## Security Architecture

### Current Security

- âœ… Database credentials via environment variables
- âœ… API key management via .env
- âœ… CORS middleware (configurable)
- âœ… SQL injection protection (SQLAlchemy)
- âœ… Input validation (Pydantic)
- âš ï¸ No authentication (local use)

### Production Security Checklist

- [ ] Add JWT authentication
- [ ] Enable HTTPS with SSL certificates
- [ ] Implement rate limiting (per user/IP)
- [ ] Use secrets manager (AWS Secrets Manager, Vault)
- [ ] Enable database encryption at rest
- [ ] Set up VPC/network isolation
- [ ] Implement audit logging
- [ ] Add API key rotation
- [ ] Enable CSP headers
- [ ] Set up monitoring & alerting

## Configuration Management

### Environment Variables

All configuration via `.env` file:

```env
# Database
DATABASE_URL=postgresql://...
POSTGRES_USER=...
POSTGRES_PASSWORD=...

# Redis
REDIS_URL=redis://...
CELERY_BROKER_URL=redis://...

# OpenAI
OPENAI_API_KEY=...
OPENAI_MODEL=gpt-4o-mini
```

### Application Config

**File**: `backend/app/config.py`

```python
class Settings(BaseSettings):
    # Database
    DATABASE_URL: str
    
    # Scraping
    MAX_SCRAPING_DEPTH: int = 3
    MAX_PAGES_PER_DOMAIN: int = 50
    RATE_LIMIT_DELAY: float = 1.0
    
    # AI
    OPENAI_API_KEY: str
    OPENAI_MODEL: str = "gpt-4o-mini"
```

## Development Workflow

```
1. Edit code
   â””â”€â–¶ Auto-reload (FastAPI/Streamlit)

2. Run tests
   â””â”€â–¶ pytest backend/tests/

3. Commit changes
   â””â”€â–¶ Git commit

4. Rebuild if needed
   â””â”€â–¶ docker-compose build

5. Deploy
   â””â”€â–¶ docker-compose up -d
```

## Monitoring & Observability

### Current Logging

- FastAPI: uvicorn logs
- Celery: task execution logs
- Database: PostgreSQL logs
- Redis: connection logs

### Recommended Additions

- **Metrics**: Prometheus + Grafana
- **Tracing**: Jaeger or DataDog
- **Alerting**: PagerDuty or Slack integration
- **Log Aggregation**: ELK Stack or Loki

---

This architecture is designed for:
- âœ… Easy local development
- âœ… Simple deployment (Docker Compose)
- âœ… Horizontal scalability
- âœ… Maintainability
- âœ… Extensibility

